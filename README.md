# CCTR: Test-Aware Cognitive Complexity for Unit Tests

This repository provides the full implementation, datasets, and replication package for the paper:

> **Rethinking Cognitive Complexity for Unit Tests: Toward a Readability-Aware Metric Grounded in Developer Perception** 
> Accepted at the **New Ideas and Emerging Results (NIER) Track**, *ICSME 2025 – International Conference on Software Maintenance and Evolution*


We introduce **CCTR** (Cognitive Complexity for Test Readability), a novel metric tailored to **quantify the structural and semantic complexity of unit test code**. CCTR addresses the shortcomings of traditional metrics such as Cognitive Complexity and Cyclomatic Complexity when applied to test suites—especially those generated by LLMs or search-based tools like EvoSuite.

---

## Why CCTR?

Most widely used complexity metrics fail to reflect the *actual comprehension effort* involved in reading test code. Specifically:

- **Deeply nested but trivial test code** receives high scores.
- **Semantically complex tests** with mocks, assertions, and parameterization are scored as 0.
- **LLM- or EvoSuite-generated tests** exhibit structural patterns that existing metrics ignore.

**CCTR** fills this gap by incorporating **test-specific constructs** into a single, interpretable score:
- Control-flow nesting (Sonar-inspired)
- Assertion density (e.g., `assertEquals`, `assertTrue`)
- Mocking constructs (`mock`, `verify`, `when`)
- Annotation semantics (`@Test`, `@ParameterizedTest`, etc.)

CCTR is designed to better align with *developer intuition*, *empirical findings on test readability*, and *semantic effort*.

---

## Repository Structure

```plaintext
CCTR/
├── Approach/
│   ├── Deeply_Nested_Trivial_Code/
│   ├── EvoSuite_Generated/
│   ├── GPT_Generated/
│   ├── compare_complexity_all.py
│   ├── readability_model/
│   ├── rulesets/
│   └── outputs/
├── Dataset/
│   ├── Defects4J_dataset.json
│   └── SF110_dataset.json
├── EvoSuite-Generated-Test-Suite/
│   ├── Generated-Test-Suite/
│   ├── Generated-Test-Suite-Code/
│   └── Scripts/
├── LLM-Generated-Test-Suite/
│   ├── Generated-Test-Suite/
│   ├── Generated-Test-Suite-Code/
│   ├── Generated-Test-Suite-Compilable/
│   ├── Generated-Test-Suite-Raw/
│   └── Scripts/
├── Motivating-Example/
├── Results/
└── README.md
```

---

## Quick Start

### Requirements

- **Java 11+** – for PMD and readability analysis
- **Python 3.8+**
- Unix-like environment (Linux, macOS, WSL recommended)

### Installation & Execution

1. Clone the repository:
   ```bash
   git clone https://github.com/our-repo/CCTR.git
   cd CCTR
   ```

2. Install Python dependencies:
   ```bash
   cd Approach/
   pip install -r requirements.txt
   ```

3. Run the motivating example:
   ```bash
   python3 compare_complexity_all.py
   ```

This will generate:
```
outputs/complexity_summary.csv
```

### Sample Output

```
File                          | Sonar | CCTR  | PMD Cog | Cyclo | Readability
------------------------------------------------------------------------------------
ExampleTest.java              |   12  |   12  |    6    |   4   | 0.95
CommandLine_ESTest.java       |    0  |   35  |    0    |  17   | 0.54
CommandLineTest.java          |    0  |   12  |    0    |  13   | 0.77
```

---

## CCTR Metric

The CCTR metric is defined as:

```text
CCTR = α·N + β·A + γ·M + δ·T
```

Where:
- **N**: Control-flow nesting complexity (Sonar-style)
- **A**: Number of assertion-related statements
- **M**: Mocking-related constructs (e.g., `mock`, `verify`)
- **T**: Annotation semantics (e.g., `@Test`, `@ParameterizedTest`)

**Default weights:** α = β = γ = δ = 1.0 (interpretable & empirically balanced)

### Key Properties

- **Test-aware**: Recognizes assertion/mocking/annotation patterns
- **Lightweight**: Fast, static, and analyzable at scale
- **Intuitive**: Aligns with developer perception
- **Extensible**: Adaptable with tunable weights

---

## Experimental Highlights

### Motivating Example

Compare three test suites for the same class:
- Deeply nested trivial logic (manual)
- GPT-4o-generated suite
- EvoSuite-generated suite

> Traditional metrics rate all three similarly. **CCTR differentiates them clearly**, aligning with structural and semantic expectations.

### Large-Scale Evaluation

**15,750 test suites** evaluated across:
- **Defects4J** (147 classes)
- **SF110** (203 classes)
- Tools: **GPT-4o**, **Mistral-L**, **EvoSuite**

Key findings:
- SonarSource/PMD Cognitive Complexity gives 0 to >99% of LLM tests
- CCTR provides a **broader and more meaningful distribution**
- **EvoSuite tests score higher**, due to fragmentation and assertion overload

### Benchmark Datasets

- **Defects4J**: Faulty real-world Java classes
- **SF110**: Broad Java class benchmark

---

## Results Summary

- CCTR **captures effort** beyond control-flow: assertions, mocking, annotations
- Better **differentiation between test styles**
- **Scales to LLM-generated code**
- Stronger correlation with **perceived readability** than PMD/Sonar

Detailed metrics are provided in the `Results/` folder.

---

## References

- Scalabrino et al. (2018) – *Code Readability Model*
- Winkler et al. (2024) – *Test Code Readability & Developer Perception*
- Guerra et al. (2024) – *Impact of Annotations on Readability*
- SonarSource – [Cognitive Complexity White Paper](https://www.sonarsource.com/docs/CognitiveComplexity.pdf)
- PMD Static Analysis – [https://pmd.github.io](https://pmd.github.io)
- Tree-sitter Parser – [https://tree-sitter.github.io](https://tree-sitter.github.io)

---

